---
layout: post
title: 自然语言学习简要路径  
date: 2024-07-08 08:57:00-0400
description: 如题
tags: NLP
categories: NLP
giscus_comments: true
related_posts: false
---


大模型相关的 [LLM-learning](https://github.com/Azyka/LLM-Learning/tree/main)


再来点个人建议， 指出几个资源、列举几个方向的几篇论文

## NLP入门课程

如台大李宏毅、 西湖大学张岳(B站有)

大模型年代，可能会倾向于先学大模型知识

## 大模型方面

课程待找

视频资源 [跟李沐学AI 合集 AI论文精读](https://space.bilibili.com/1567748478/channel/collectiondetail?sid=32744)

NLP 必读论文 对应视频：

- [Transformer](https://www.bilibili.com/video/BV1pu411o7BE/)
- 关键的 Residual 残差网络 [部分1](https://www.bilibili.com/video/BV1Fb4y1h73E/) 和 [部分2](https://www.bilibili.com/video/BV1P3411y7nn/)。
- 重要方向 [BERT](https://www.bilibili.com/video/BV1PL411M7eQ/)
- 大语言模型的 [GPT 1 2 3](https://www.bilibili.com/video/BV1AF411b7xQ/)
- [Chain of thoughts](https://www.bilibili.com/video/BV1t8411e7Ug/)
- ChatGPT背后的技术 [InstructGPT](https://www.bilibili.com/video/BV1hd4y187CR/)
- 文本-图像多模态的基础 [CLIP](https://www.bilibili.com/video/BV1SL4y1s7LQ/)
- 微调/精调  [LoRA](https://www.bilibili.com/video/BV1tthPeFEWb/) 

# 论文主要划分 

## 基础结构

- Attention is all you need
- ResNet
- BERT
- GPT 1 2 3
- T5 其实就是 Transformer
- GLM General Language Model Pretraining其实就是某种 Transformer（ 
- ViT Transformer在图像
- RWKV?
- Mamba?

## 大模型推理

- Chain of thoughts
- self-consistency CoT
- Let's think step by step(zero-shot reasoner)
- 等等
- 列太多就成 awesome-XXX 了

## 微调 PEFT 

parameter effecient fine tuning

主要是 LoRA, Adapter 和 其他

### LoRA


### Adapter



