<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="cn"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://yangxlai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yangxlai.github.io/" rel="alternate" type="text/html" hreflang="cn"/><updated>2025-02-03T08:44:21+00:00</updated><id>https://yangxlai.github.io/feed.xml</id><title type="html">杨秀隆在CCNU</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">教师手机或平板控制电脑，当翻页笔用甚至写字</title><link href="https://yangxlai.github.io/blog/2099/%E6%95%99%E5%B8%88%E6%89%8B%E6%9C%BA%E6%88%96%E5%B9%B3%E6%9D%BF%E5%BD%93%E7%BF%BB%E9%A1%B5%E7%AC%94%E6%88%96%E6%89%8B%E5%86%99%E6%9D%BF%E6%8E%A7%E5%88%B6%E7%94%B5%E8%84%91/" rel="alternate" type="text/html" title="教师手机或平板控制电脑，当翻页笔用甚至写字"/><published>2099-12-31T12:59:00+00:00</published><updated>2099-12-31T12:59:00+00:00</updated><id>https://yangxlai.github.io/blog/2099/%E6%95%99%E5%B8%88%E6%89%8B%E6%9C%BA%E6%88%96%E5%B9%B3%E6%9D%BF%E5%BD%93%E7%BF%BB%E9%A1%B5%E7%AC%94%E6%88%96%E6%89%8B%E5%86%99%E6%9D%BF%E6%8E%A7%E5%88%B6%E7%94%B5%E8%84%91</id><content type="html" xml:base="https://yangxlai.github.io/blog/2099/%E6%95%99%E5%B8%88%E6%89%8B%E6%9C%BA%E6%88%96%E5%B9%B3%E6%9D%BF%E5%BD%93%E7%BF%BB%E9%A1%B5%E7%AC%94%E6%88%96%E6%89%8B%E5%86%99%E6%9D%BF%E6%8E%A7%E5%88%B6%E7%94%B5%E8%84%91/"><![CDATA[<p>众所周不知， 翻页笔能避免我们讲课时站在讲台上被鼠标限制。 当然还有纯板书的大神级教师， 但板书应该也不太好离开黑板</p> <ol> <li>便宜的如激光翻页笔 十几二十块钱（都快和退货运费差不多了）， 功能少， 激光在幕布上不一定能看清。</li> <li>高阶的一般差不多200以上， 绿联179（两个月后基本不需要了）， 罗技 599， 功能强了很多， 空中键鼠。 但确实贵。</li> </ol> <p>在这我推荐两个方案， 可以使用手机代替高阶翻页笔， 以及平板代替翻页笔加手写板——一手拿平板，一手写字，写得漂亮的可以直接避免粉笔和擦黑板了（我字丑主要在PPT上写）。 另外， 使用会议或直播软件如腾讯会议、或智慧屏投屏， 可以将ipad 直接投屏到笔记本或智慧屏， 显示到大屏上。 不过主要看是喜欢ipad里的软件还是 windows里的软件。</p> <h1 id="超星学习通">超星学习通</h1> <p>使用方法：</p> <ol> <li> <p>注册并绑定教师单位——如何审核，可以在注册后，首页-我的课程-新建课程， 没有绑定时，它会在你点建课时要求身份审核（审核两天？）。</p> </li> <li> <p>点击课程-投屏-在电脑网页按说明操作即可。</p> </li> </ol> <p>要求： 注册教师（绑定），并开课，</p> <p>好处：</p> <ol> <li>教学资源多（跟投屏没什么关系）</li> <li>手机直接当空中鼠标， 重力感应，不需要激光指向。</li> <li>用ipad屏幕大一点， 可以手写(手指或apple pencil)， 当作空中鼠标则不是很方便。</li> <li>PPT写的备注也能显示， 需要时可以读。</li> </ol> <p>限制：</p> <ol> <li>只能在那一个网页上，</li> <li>只能使用所开课程的课件，</li> <li>不能打开其他软件，</li> <li>需要网络，不用太快，但也不能太烂</li> </ol> <h1 id="easy-canvas">easy canvas</h1> <p>这app本来是用于美术时当手写板的</p> <p>理工科需要板书时， 手机屏幕太小， 该高亮的文字写PPT时就高亮处理就行了， 感觉激光笔、聚光灯这些功能也不是非常必要。 板书非常重要。</p> <p>使用方法：</p> <ol> <li>平板和电脑上分别安装 easy canvas 软件</li> <li>两端运行， 使用usb 有线连接， 成功在ipad上显示， 在电脑端选择显示方式。 我在有线连接时基本没遇到过问题。</li> <li>电脑和平板连接至同一wifi， 拔掉usb数据线， 仍能显示——可能要重启电脑一回</li> <li>校园网应该不算同一wifi（也可能是我没重启电脑）， 笔记本自带热点、iphone手机热点好像也不行（仍然可能是我没正确重启）， 我用 connectify-me 创建热点，缺点是免费版一个小时得重启热点。 换设备、换wifi 可能都得重启电脑。</li> <li>投屏后， 就可以用 ipad pencil和手指点击、写字了。</li> </ol> <p>ppt放映时，ipad pencil默认就是写字， 个人推荐使用荧光笔加红色， 默认的笔 太细了， 可能看不清。 不放映时， PPT也可以在菜单栏的绘图进行绘制</p> <p>屏幕直接书写可以使用 gink 开源软件</p> <p>其他功能， 可以通过平板打开各种软件， 只是打字没键盘。</p> <p>限制：</p> <ol> <li>connectify-me时长限制， 有课间休息倒也不算限制。 讲座一般也不用自己的电脑，用不了。</li> </ol> <h1 id="思考">思考</h1> <p>讲台、 板书、 PPT、 激光笔（指示位置） 之间总存在一定取舍。</p> <p>教师要走下讲台、走近学生的话， 板书就比较麻烦， 必须得走回去。</p> <p>实现远程控制了， 学生看无人的高亮 或 板书， 对比 有人有教师手指PPT位置、手写的板书， 效果如何？</p> <p>就是问， 学生是看走下讲台的教师呢， 还是看台上不断书写的板书？ 会不会分神。</p> <p>这些应该是教育学早有争论或定论的结果了。</p>]]></content><author><name></name></author><category term="其他"/><category term="其他"/><summary type="html"><![CDATA[超星学习通或easy canvas控制电脑]]></summary></entry><entry><title type="html">docker GPU报错</title><link href="https://yangxlai.github.io/blog/2025/docker_GPUs/" rel="alternate" type="text/html" title="docker GPU报错"/><published>2025-02-02T12:57:00+00:00</published><updated>2025-02-02T12:57:00+00:00</updated><id>https://yangxlai.github.io/blog/2025/docker_GPUs</id><content type="html" xml:base="https://yangxlai.github.io/blog/2025/docker_GPUs/"><![CDATA[<h1 id="报错-gpu">报错 GPU</h1> <p>docker 报错, 带 <code class="language-plaintext highlighter-rouge">--gpus</code>时报错， 不带该参数使用CPU不报错</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
</code></pre></div></div> <p>deepseek回答要安装 <strong>NVIDIA Docker工具包</strong></p> <p>现链接如下</p> <p>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</p> <p>需要安装 cuda\cudatoolkit， cuda安装 有点混乱， cudatoolkit安装方案也不少， 比如 anaconda就可以</p> <h1 id="报错-libnvidia-mlso1">报错 libnvidia-ml.so.1</h1> <p>前面安装正常， 有些docker镜像正常， 但有的失败， 报错 <code class="language-plaintext highlighter-rouge">load library failed: libnvidia-ml.so.1: cannot open shared object file</code></p> <p>查询到 https://github.com/NVIDIA/nvidia-container-toolkit/issues/305 链接，最后一层， 建议重装</p> <p>just reinstalling docker-ce using sudo apt-get install –reinstall docker-ce</p> <h1 id="a100的nv-fabricmanager">A100的nv-fabricmanager</h1> <p>不敢动~ 所以没试出方案。</p> <p>另外， 个人所购服务器 启动docker极快， 该服务器明明更强，但很慢，原因不明。</p>]]></content><author><name></name></author><category term="开发工具"/><category term="docker"/><summary type="html"><![CDATA[如题]]></summary></entry><entry><title type="html">Ollama下载模型路径存储位置及权限</title><link href="https://yangxlai.github.io/blog/2025/ollama_model_path/" rel="alternate" type="text/html" title="Ollama下载模型路径存储位置及权限"/><published>2025-01-31T12:57:00+00:00</published><updated>2025-01-31T12:57:00+00:00</updated><id>https://yangxlai.github.io/blog/2025/ollama_model_path</id><content type="html" xml:base="https://yangxlai.github.io/blog/2025/ollama_model_path/"><![CDATA[<p>根据网络上的建议，在ubuntu系统里， 我已经完成了以下内容</p> <h1 id="1-配置文件">1 配置文件</h1> <p>创建配置文件：</p> <p>/etc/systemd/system/ollama.service.d/override.conf</p> <p>内容:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Service]
Environment="OLLAMA_MODELS=/mnt/share/ollama_models"
</code></pre></div></div> <p>df -h 可得：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/dev/nvme0n1p1  3.6T  103G  3.3T   3% /mnt/share
</code></pre></div></div> <p>运行</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo systemctl daemon-reload
sudo systemctl restart ollama
sudo systemctl status ollama
</code></pre></div></div> <h1 id="出现错误">出现错误</h1> <p>检查问题， 使用 ` journalctl -xe`</p> <p>错误内容显示为：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1月 31 12:01:40 4x3090 ollama[3094883]: Error: mkdir /mnt/share/ollama_models: permission denied
</code></pre></div></div> <h1 id="分析错误">分析错误</h1> <p>permission denied 是明显的 <strong>权限错误</strong></p> <p>查看ollama程序的执行用户， 我习惯使用 <code class="language-plaintext highlighter-rouge">ps aux | grep ollama</code>， 得到 用户名 ollama：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama   3095129  0.1  0.0 19697324 63576 ?      Ssl  12:02   0:01 /usr/local/bin/ollama serve
</code></pre></div></div> <p>再查看路径的执行权限， 从 /mnt,/mnt/share,/mnt/share/ollama_models 每层看下来：</p> <p>/mnt 一般不影响 ` drwxr-xr-x 6 root root 4096 7月 26 2024 mnt `</p> <p>/mnt/share 的结果</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>drwxrwx--- 9 root sharegroup 4096 1月  31 11:44 share
</code></pre></div></div> <p>因此使用 <code class="language-plaintext highlighter-rouge">sudo usermod -a -G sharegroup ollama</code></p> <p><em>后续步骤不确定是否多余</em></p> <p>创建 /mnt/share/ollama_models， 查看权限， 我已经修改</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>drwxrwxrwx 3 ollama ollama      4096 1月  31 12:02 ollama_models
</code></pre></div></div> <p>执行的命令为以下两条：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo chown -R ollama:ollama /mnt/share/ollama_models    # 修改用户
sudo chmod -R 755  /mnt/share/ollama_models      # 修改权限， 别人用的是755， 用777 好像太放开了~
</code></pre></div></div>]]></content><author><name></name></author><category term="NLP"/><category term="NLP"/><summary type="html"><![CDATA[如题]]></summary></entry><entry><title type="html">将HF模型转GGUF格式用于Ollama</title><link href="https://yangxlai.github.io/blog/2025/HF_to_GGUF-Ollama/" rel="alternate" type="text/html" title="将HF模型转GGUF格式用于Ollama"/><published>2025-01-04T12:57:00+00:00</published><updated>2025-01-04T12:57:00+00:00</updated><id>https://yangxlai.github.io/blog/2025/HF_to_GGUF-Ollama</id><content type="html" xml:base="https://yangxlai.github.io/blog/2025/HF_to_GGUF-Ollama/"><![CDATA[<h1 id="将hugginface里的模型转成-gguf">将hugginface里的模型转成 GGUF</h1> <p>来源 https://github.com/ggerganov/llama.cpp/discussions/2948</p> <p>1、 下载 llama.cpp 代码</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/ggerganov/llama.cpp.git
</code></pre></div></div> <p>创建环境， 安装</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -r llama.cpp/requirements.txt
</code></pre></div></div> <p>2、 下载模型</p> <p>2025年1月， 使用 https://hf-mirror.com/ 下载模型</p> <p>3、 转换</p> <p>转换命令如下：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python convert_hf_to_gguf.py /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-merged --outfile /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v1.gguf --outtype q8_0
./llama-quantize ./新模型路径/新模型名.gguf  ./输出路径/4位gguf格式文件名.gguf Q4_K_M 采用4位  如果8位就Q8_K_M
--outtype f16 (16 bit) or --outtype f32 (32 bit) 
</code></pre></div></div> <p>4、 ollama 使用</p> <p>https://github.com/ollama/ollama?tab=readme-ov-file#import-from-gguf</p> <p>4.1 创建 Modelfile文件 里面写上 <code class="language-plaintext highlighter-rouge">FROM ./vicuna-33b.Q4_0.gguf</code><br/> 4.2 <code class="language-plaintext highlighter-rouge">ollama create model_name -f Modelfile</code><br/> 4.3 <code class="language-plaintext highlighter-rouge">ollama run model_name</code></p>]]></content><author><name></name></author><category term="NLP"/><category term="NLP"/><summary type="html"><![CDATA[如题]]></summary></entry><entry><title type="html">硕博面试小建议——讲好故事</title><link href="https://yangxlai.github.io/blog/2024/%E7%A1%95%E5%8D%9A%E9%9D%A2%E8%AF%95%E5%B0%8F%E5%BB%BA%E8%AE%AE/" rel="alternate" type="text/html" title="硕博面试小建议——讲好故事"/><published>2024-12-31T15:59:00+00:00</published><updated>2024-12-31T15:59:00+00:00</updated><id>https://yangxlai.github.io/blog/2024/%E7%A1%95%E5%8D%9A%E9%9D%A2%E8%AF%95%E5%B0%8F%E5%BB%BA%E8%AE%AE</id><content type="html" xml:base="https://yangxlai.github.io/blog/2024/%E7%A1%95%E5%8D%9A%E9%9D%A2%E8%AF%95%E5%B0%8F%E5%BB%BA%E8%AE%AE/"><![CDATA[<p>经验有限， 仅就本校的一些流程和（部分）学生面试时疏忽的内容进行一些提醒</p> <p>面试核心就是展示和回答”Why me 为什么选我”</p> <p>最大挑战就是怎么像印度人（当然还有美国人）一样， 非常自信， 非常会说, 6分及格能吹成12分。将来写论文或别的也差不多。</p> <p>两句话总结</p> <ol> <li>自我介绍中英文都要准备，临考前背也要背下来， 最好也准备点 “我很喜欢这学校，原因啥啥啥”的话， 自我介绍里可以带一两句， 额外准备一点。</li> <li>科研和项目介绍， 一两句话介绍完项目做什么， 接下来就是重点强调自己负责什么内容。 突出你的数学好（尤其是高阶数学，不是高数线代），可以； 也可以突出 你对项目全局和细节的掌握程度， 或者说工作量（方便量化）， 平凡项目吹出花来更是种能力。</li> </ol> <p>下面内容就没必要看了。</p> <h2 id="英语">英语</h2> <p>中国人的英语听说读写里， 听说写确实都弱很多（me too）</p> <p>还是要练一练的， 不管是为了出国、还是未来读研、正在读研。 <a href="/blog/2024/ThoughtsOnEnglish/">一点小想法</a>， 其实就是大胆去做，一天15分钟也能进步。</p> <h2 id="自我介绍">自我介绍</h2> <p>1、2分钟的自我介绍肯定要准备的， 中文和英文的都要准备， 临考前（面试）背也要背一稿</p> <h2 id="看中的亮点">看中的亮点</h2> <p>不同人看重点不相同， 以计算机/人工智能方向为例， 不外乎 学校、数学、编程能力、科研竞赛能力和经历、GPA， 留学还有基本英语成绩、推荐信。</p> <p>知乎上南大教授写的：<code class="language-plaintext highlighter-rouge">直博生=硕博连读&gt;博士研究生&gt;硕士研究生；有思想的&gt;肯干活的&gt;混日子的&gt;情商低的</code> , 出自<a href="https://www.zhihu.com/question/328202840/answer/2512378561">现在导师更喜欢保研生还是考研生？</a></p> <p>个人排序：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>独立科研（超神） ~ 数学物理顶级竞赛拿奖（超导） &gt; 熟悉高阶数学物理 &gt; 科研项目及论文发表 &gt; 高数线代概率统计 &gt; 代码能力（主要是深度学习领域比较特殊） &gt; 学校
</code></pre></div></div> <p>对于没有引路人的学生来说， 很难有这样的规划， 实际排序就可能是</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>发表优秀论文 ~ 优秀数理比赛获奖 &gt; 科研项目及论文发表 &gt; 学校学历 &gt; 其他比赛 &gt; 代码能力 &gt; 数学成绩 &gt; 计算机课程成绩 &gt; 综合GPA &gt;&gt; 一般社会实践活动
</code></pre></div></div> <p>科研或项目经历是必问， 数学基础是选问， 能在项目经历中讲清楚数学理论、展现数学基础扎实， 比项目得奖更有意义。</p> <p>就计算机而言， 如果把数学分析、高代、概率统计，以及再之上的一两门更高阶数学课的教材所有习题全部做出来了（且没忘），加上数据结构和算法（网络、操作系统和数据库看方向）， 面试中等985及以下 应该都是亮点， 不过花的时间应该是比做项目竞赛要久， 难度也高得多， 我只是这么一说。</p> <h2 id="项目讲故事">项目讲故事</h2> <p>既然是面试， 很多面试建议都会提到简历和面试都要突出“Why you”</p> <p>逻辑清晰、细节明确地讲好项目故事， 也就是项目核心或你负责的内容</p> <ul> <li>一定要熟练， 面试不会问到每行代码，但要做到相应模块、流程完全熟悉。 <br/> 例如深度学习科研项目， 针对什么不足， 创新点是什么， 也就是别人怎么做的，也能想到</li> <li>有多个项目，则不熟的项目建议不写； 实在没有项目，必须写不熟悉的项目就要好好准备项目内容、熟练。</li> </ul>]]></content><author><name></name></author><category term="其他"/><category term="其他"/><summary type="html"><![CDATA[准备面试时的基本点， 也是平时准备保研、考研可以注意的、非专业知识的内容]]></summary></entry><entry><title type="html">Docker如何用上宿主机的Clash代理</title><link href="https://yangxlai.github.io/blog/2024/docker%E5%86%85%E9%83%A8%E4%BD%BF%E7%94%A8%E5%AE%BF%E4%B8%BB%E6%9C%BA%E4%BB%A3%E7%90%86/" rel="alternate" type="text/html" title="Docker如何用上宿主机的Clash代理"/><published>2024-11-15T16:40:16+00:00</published><updated>2024-11-15T16:40:16+00:00</updated><id>https://yangxlai.github.io/blog/2024/docker%E5%86%85%E9%83%A8%E4%BD%BF%E7%94%A8%E5%AE%BF%E4%B8%BB%E6%9C%BA%E4%BB%A3%E7%90%86</id><content type="html" xml:base="https://yangxlai.github.io/blog/2024/docker%E5%86%85%E9%83%A8%E4%BD%BF%E7%94%A8%E5%AE%BF%E4%B8%BB%E6%9C%BA%E4%BB%A3%E7%90%86/"><![CDATA[<p>来源 <a href="https://williamlfang.github.io/2024-03-28-docker-%E4%BD%BF%E7%94%A8-host-%E4%BB%A3%E7%90%86/">https://williamlfang.github.io/2024-03-28-docker-%E4%BD%BF%E7%94%A8-host-%E4%BB%A3%E7%90%86/</a></p> <h1 id="问题描述">问题描述：</h1> <p>al-folio 学术主页访问谷歌学术， 没有代理会报错</p> <p>在 docker-compose.yml 文件里进行修改</p> <p>加入</p> <p>network_mode: host 和代理</p> <p>如下：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: "3"
# this file uses prebuilt image in dockerhub
services:
  jekyll:
    image: amirpourmand/al-folio:v0.12.0
    build: .
    network_mode: host
    ports:
      - 8080:8080
      - 35729:35729
    volumes:
      - .:/srv/jekyll
    environment:
      - HTTP_PROXY=http://127.0.0.1:7897
      - HTTPS_PROXY=http://127.0.0.1:7897
      - NO_PROXY=localhost,127.0.0.1
</code></pre></div></div>]]></content><author><name></name></author><category term="工具"/><category term="虚拟机,"/><category term="Docker"/><summary type="html"><![CDATA[Docker (及 docker compose)使用 宿主机 Clash]]></summary></entry><entry><title type="html">独显用于深度学习运算，核显用于屏幕显示</title><link href="https://yangxlai.github.io/blog/2024/%E7%8B%AC%E6%98%BE%E7%94%A8%E4%BA%8E%E8%BF%90%E7%AE%97%E6%A0%B8%E6%98%BE%E7%94%A8%E4%BA%8E%E5%B1%8F%E5%B9%95%E6%98%BE%E7%A4%BA/" rel="alternate" type="text/html" title="独显用于深度学习运算，核显用于屏幕显示"/><published>2024-11-14T16:40:16+00:00</published><updated>2024-11-14T16:40:16+00:00</updated><id>https://yangxlai.github.io/blog/2024/%E7%8B%AC%E6%98%BE%E7%94%A8%E4%BA%8E%E8%BF%90%E7%AE%97%E6%A0%B8%E6%98%BE%E7%94%A8%E4%BA%8E%E5%B1%8F%E5%B9%95%E6%98%BE%E7%A4%BA</id><content type="html" xml:base="https://yangxlai.github.io/blog/2024/%E7%8B%AC%E6%98%BE%E7%94%A8%E4%BA%8E%E8%BF%90%E7%AE%97%E6%A0%B8%E6%98%BE%E7%94%A8%E4%BA%8E%E5%B1%8F%E5%B9%95%E6%98%BE%E7%A4%BA/"><![CDATA[<p>来源 <a href="https://blog.csdn.net/qq_41983842/article/details/129131044">https://blog.csdn.net/qq_41983842/article/details/129131044</a></p> <h1 id="问题描述">问题描述</h1> <p>Nvidia显卡用于显示， xorg/firefox等会占用两三百兆显存， 以及 在低功率地运行显卡</p> <p>一直尝试切换到 集成显卡用于显示， 然而一直不成功。</p> <p>本文是最接近成功的一次， 主要是解释了 nvidia-settings 没有 prime profiles选项的问题。</p> <p>即</p> <ol> <li>prime-select query 可看到通常是on-demand混合模式，</li> <li>使用 sudo prime-select intel 切换成集成显卡显示的模式， 将 hdmi/DP反正就是显示器 接到 集显接口上。</li> <li>重启就可以看到 nvidia-settings 出现 prime profiles选项了， 但 nvidia-smi 显示无法连接显卡。</li> <li>在nvidia-settings的prime profiles换成 on demand</li> <li>显示器保持在集显接口不变， 重启</li> </ol> <p>我在第五步挂了， ubuntu 桌面无法启动， 光标闪， 有一次又显示 hdaudio2那个错误， 没解决。</p> <p>安全模式能正常显示~~~</p> <p>所以昨天写的是 （五笔输入也坏了）：</p> <p>almost done</p> <p>still fail</p> <p>explain something</p>]]></content><author><name></name></author><category term="工具"/><category term="ubuntu"/><summary type="html"><![CDATA[独显用于深度学习运算，核显用于屏幕显示]]></summary></entry><entry><title type="html">关于英语的乱想法</title><link href="https://yangxlai.github.io/blog/2024/ThoughtsOnEnglish/" rel="alternate" type="text/html" title="关于英语的乱想法"/><published>2024-09-27T11:59:00+00:00</published><updated>2024-09-27T11:59:00+00:00</updated><id>https://yangxlai.github.io/blog/2024/ThoughtsOnEnglish</id><content type="html" xml:base="https://yangxlai.github.io/blog/2024/ThoughtsOnEnglish/"><![CDATA[<p>经验有限， 简单、不成熟的想法</p> <p>单项每天大概15分钟的练习， 好过过于费时不去做。</p> <p>中国人的英语听说读写里， 听说写确实都弱很多（me too）</p> <h2 id="听力">听力</h2> <p>听写其实资源很多、也好进行， 坚持比如每天或一周五天各15-20分钟， 听写一分钟的材料， 不熟练的坚持听写完1分钟的材料可能要花半小时， 心都累了。不如12-15分钟，能听写多少是多少， 再来3、5分钟对比，哪里没听懂。 能看到自己的进步， 比如 从慢速写三五句，到常速写完。</p> <h2 id="口语">口语</h2> <p>口语，以前难， 现在有大模型了， 还可以对话和较好地评判了。</p> <p>发音方面 英语流利说 是不错。</p> <p>练口语不一定看重发音， 印度人平均口音也很重，不影响他们说得流畅， data mining 发成 “打他money” 影响不大。</p> <ol> <li>抽签做题， 让大模型随机出个话题（读题或听题）， 直接英语口语即兴回答</li> <li>一开始，可能 10秒钟磕磕绊绊凑不出一句话， 那就去写稿， 写英语的稿子， 然后用大模型改掉语病和错误用词， 以及要求口语化。</li> <li>背稿、读出来。</li> <li>翻译，看中文，逐渐提升语感、反应速度。</li> <li>即兴回答要多练， 具体话题范围不定，大概就是生活化、专业化。有时可能不只是英语答不上来，而是用中文也答不上来， 所以得多尝试各种话题。比如我喜欢宅，那运动、风景、旅游甚至家乡话题，我用中文都说不上来。</li> </ol> <p>可用的app， 文心一言（百度的）和豆包（字节的？）好像都支持口语问答。</p> <h2 id="写作">写作</h2> <p>原则上和口语类似， 而且基本只用考虑学术写作， 日常英语写作比如写邮件用大模型就很方便了。</p> <p>学术写作总体</p> <ol> <li>全文结构。 包括不同学科方向、期刊会议的写作风格， 比如一般人工智能顶会新方法论文就是<code class="language-plaintext highlighter-rouge">导论-(相关研究)-方法-实验-讨论和结论</code>， 但生物信息学有些方法理论不是重点， 放后面或就是放附录。</li> <li>故事逻辑。 导论</li> <li>细节句子。</li> </ol> <h3 id="阅读">阅读</h3> <p>读论文 略读、精读， 各路大神的说法都差不多 <a href="https://www.bilibili.com/video/BV1H44y1t75x/">李沐：如何读论文</a>。 不过我觉得大神的经验可能没考虑论文还没读超过3、5篇的新手？</p> <p>GRE的阅读大概是词汇量因素</p>]]></content><author><name></name></author><category term="其他"/><category term="其他"/><summary type="html"><![CDATA[英语听说读写常规化锻炼]]></summary></entry><entry><title type="html">Docker如何用上Clash代理</title><link href="https://yangxlai.github.io/blog/2024/docker%E4%BB%A3%E7%90%86/" rel="alternate" type="text/html" title="Docker如何用上Clash代理"/><published>2024-09-25T16:40:16+00:00</published><updated>2024-09-25T16:40:16+00:00</updated><id>https://yangxlai.github.io/blog/2024/docker%E4%BB%A3%E7%90%86</id><content type="html" xml:base="https://yangxlai.github.io/blog/2024/docker%E4%BB%A3%E7%90%86/"><![CDATA[<p>来源 <a href="https://blog.haohtml.com/archives/31298">https://blog.haohtml.com/archives/31298</a></p> <p>通过直接设置 http_proxy 和 https_proxy 这两个环境变量是不可行的</p> <ol> <li>设置docker服务代理</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo mkdir -p /etc/systemd/system/docker.service.d/
sudo vim /etc/systemd/system/docker.service.d/http-proxy.conf
</code></pre></div></div> <p>将以下内容写入 http-proxy.conf 文件</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Service]
Environment="HTTP_PROXY=http://127.0.0.1:7897"
Environment="HTTPS_PROXY=http://127.0.0.1:7897"
Environment="ALL_PROXY=socks5://127.0.0.1:7897"
Environment="NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp,.docker.io,.docker.com"
</code></pre></div></div> <p>上面代理地址是本机开启的代理服务监听端口，如果代理服务在局域网内的其它机器上的话，需要更换为其 ip 地址和端口号。环境变量 NO_PROXY 表示不使用代理的域名或IP。</p> <ol> <li>重启 docker 服务</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl daemon-reload
systemctl restart docker
</code></pre></div></div> <ol> <li>验证</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@ubuntu:~# systemctl show --property=Environment docker
Environment=HTTP_PROXY=http://127.0.0.1:7890 HTTPS_PROXY=http://127.0.0.1:7890 ALL_PROXY=socks5://127.0.0.1:7890 NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp,.docker.io,.docker.com
</code></pre></div></div>]]></content><author><name></name></author><category term="工具"/><category term="虚拟机,"/><category term="Docker"/><summary type="html"><![CDATA[Docker (及 docker compose)使用 Clash 下载镜像 pull]]></summary></entry><entry><title type="html">Ollama使用遇到的问题</title><link href="https://yangxlai.github.io/blog/2024/Ollama%E4%BD%BF%E7%94%A8/" rel="alternate" type="text/html" title="Ollama使用遇到的问题"/><published>2024-07-20T12:57:00+00:00</published><updated>2024-07-20T12:57:00+00:00</updated><id>https://yangxlai.github.io/blog/2024/Ollama%E4%BD%BF%E7%94%A8</id><content type="html" xml:base="https://yangxlai.github.io/blog/2024/Ollama%E4%BD%BF%E7%94%A8/"><![CDATA[<h1 id="ollama-和-open-webui-的连接">Ollama 和 Open-webui 的连接</h1> <p>Open-webui 使用 docker， ollama 不使用 docker</p> <p>在Admin Panel的Settings-Ollama API 里 填入 <code class="language-plaintext highlighter-rouge">http://host.docker.internal:11434</code> (注： 不一定成功)</p> <p>来源 https://github.com/open-webui/open-webui/issues/377#issuecomment-1878085031</p> <h1 id="gguf">gguf</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python convert_hf_to_gguf.py /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-merged --outfile /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v1.gguf --outtype q8_0
./llama-quantize ./新模型路径/新模型名.gguf  ./输出路径/4位gguf格式文件名.gguf Q4_K_M 采用4位  如果8位就Q8_K_M
</code></pre></div></div>]]></content><author><name></name></author><category term="NLP"/><category term="NLP"/><summary type="html"><![CDATA[如题]]></summary></entry></feed>